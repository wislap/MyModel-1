"""
ä½¿ç”¨ ModelScope åŠ è½½æ„å›¾æ ‡æ³¨æ•°æ®é›†

æ•°æ®é›†ï¼šäº¤äº’åœºæ™¯ä¸­çš„å¥å­æ„å›¾æ ‡æ³¨æ•°æ®
æ•°æ®é›†é“¾æ¥: https://www.modelscope.cn/datasets/DatatangBeijing/47811Sentence-IntentionAnnotationDataInInteractiveScenes/quickstart

è¿™ä¸ªæ•°æ®é›†æ›´é€‚åˆ Router æ¨¡å‹è®­ç»ƒï¼ŒåŒ…å«ï¼š
- äº¤äº’åœºæ™¯ä¸­çš„å¥å­
- æ„å›¾æ ‡æ³¨
- ç¬¦åˆå·¥å…·è·¯ç”±/æ„å›¾è¯†åˆ«ä»»åŠ¡éœ€æ±‚
"""

def load_intention_dataset():
    """åŠ è½½æ„å›¾æ ‡æ³¨æ•°æ®é›†"""
    # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®é›† ID
    dataset_ids = [
        "DatatangBeijing/47811Sentence-IntentionAnnotationDataInInteractiveScenes",
        "modelscope/DatatangBeijing_47811Sentence-IntentionAnnotationDataInInteractiveScenes",
        "47811Sentence-IntentionAnnotationDataInInteractiveScenes",
    ]
    
    dataset_url = "https://www.modelscope.cn/datasets/DatatangBeijing/47811Sentence-IntentionAnnotationDataInInteractiveScenes/quickstart"
    print(f"æ•°æ®é›†é¡µé¢: {dataset_url}")
    
    # æ–¹æ³•1: ä½¿ç”¨ ModelScope Dataset ç±»ï¼ˆModelScope åŸç”Ÿæ–¹å¼ï¼‰
    try:
        from modelscope.hub.sdk import HubApi
        from modelscope import MsDataset
        print("\nå°è¯•ä½¿ç”¨ ModelScope MsDataset åŠ è½½...")
        for dataset_id in dataset_ids:
            try:
                # ä½¿ç”¨ ModelScope çš„ MsDataset
                dataset = MsDataset.load(
                    dataset_id,
                    namespace='DatatangBeijing',
                    split='train',  # æˆ– 'test', 'validation'
                )
                print(f"æ•°æ®é›†åŠ è½½æˆåŠŸ!")
                return dataset
            except Exception as e:
                print(f"  {dataset_id} å¤±è´¥: {e}")
                continue
    except ImportError:
        print("ModelScope MsDataset ä¸å¯ç”¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
    except Exception as e:
        print(f"ModelScope MsDataset åŠ è½½å¤±è´¥: {e}")
    
    # æ–¹æ³•2: ä½¿ç”¨ datasets åº“ï¼ˆHuggingFace æ ¼å¼ï¼‰
    try:
        from datasets import load_dataset
        print("\nå°è¯•ä½¿ç”¨ datasets åº“åŠ è½½...")
        for dataset_id in dataset_ids:
            try:
                print(f"  å°è¯•æ•°æ®é›† ID: {dataset_id}")
                dataset = load_dataset(
                    dataset_id,
                    cache_dir="./datasets"
                )
                print(f"æ•°æ®é›†åŠ è½½æˆåŠŸ!")
                return dataset
            except Exception as e:
                print(f"  å¤±è´¥: {str(e)[:100]}...")
                continue
    except ImportError:
        print("datasets åº“æœªå®‰è£…")
    
    # æ–¹æ³•3: ä½¿ç”¨ ModelScope snapshot_downloadï¼ˆä¸‹è½½åŸå§‹æ–‡ä»¶ï¼‰
    try:
        from modelscope import snapshot_download
        print("\nå°è¯•ä½¿ç”¨ ModelScope snapshot_download ä¸‹è½½åŸå§‹æ–‡ä»¶...")
        for dataset_id in dataset_ids:
            try:
                dataset_path = snapshot_download(
                    dataset_id,
                    cache_dir="./datasets"
                )
                print(f"æ•°æ®é›†æ–‡ä»¶å·²ä¸‹è½½åˆ°: {dataset_path}")
                print("æ³¨æ„: è¿™æ˜¯åŸå§‹æ–‡ä»¶ï¼Œéœ€è¦æ‰‹åŠ¨è§£æ")
                return dataset_path
            except Exception as e:
                print(f"  {dataset_id} å¤±è´¥: {str(e)[:100]}...")
                continue
    except ImportError as e:
        print(f"ModelScope å¯¼å…¥é”™è¯¯: {e}")
    
    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œæä¾›è¯¦ç»†è¯´æ˜å’Œæ›¿ä»£æ–¹æ¡ˆ
    print("\n" + "=" * 60)
    print("âš ï¸  è‡ªåŠ¨åŠ è½½å¤±è´¥")
    print("=" * 60)
    print("å¯èƒ½çš„åŸå› :")
    print("1. æ•°æ®é›† ID ä¸æ­£ç¡®æˆ–éœ€è¦ç‰¹æ®Šæƒé™")
    print("2. æ•°æ®é›†å¯èƒ½åªåœ¨ ModelScope ç½‘ç«™æä¾›ï¼Œä¸åœ¨ Hub ä¸Š")
    print("3. éœ€è¦ç™»å½• ModelScope è´¦å·æˆ–ç”³è¯·è®¿é—®æƒé™")
    print("\nå»ºè®®æ“ä½œ:")
    print(f"1. è®¿é—®æ•°æ®é›†é¡µé¢: {dataset_url}")
    print("2. æŸ¥çœ‹é¡µé¢ä¸Šçš„ 'å¿«é€Ÿå¼€å§‹' æˆ– 'Quick Start' éƒ¨åˆ†")
    print("3. å¤åˆ¶é¡µé¢ä¸Šçš„æ­£ç¡®æ•°æ®é›† ID å’ŒåŠ è½½ä»£ç ")
    print("4. å¦‚æœéœ€è¦ï¼Œå…ˆç™»å½• ModelScope è´¦å·")
    print("5. æˆ–è€…ç›´æ¥ä¸‹è½½æ•°æ®é›†æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œç„¶åæ‰‹åŠ¨åŠ è½½")
    print("\næ›¿ä»£æ–¹æ¡ˆ:")
    print("- ä½¿ç”¨å…¶ä»–ä¸­æ–‡æ„å›¾è¯†åˆ«æ•°æ®é›†ï¼ˆå¦‚ CLUE å­ä»»åŠ¡ï¼‰")
    print("- ä½¿ç”¨è‹±æ–‡æ•°æ®é›†ï¼ˆSNIPSã€ATISï¼‰è¿›è¡Œé¢„è®­ç»ƒ")
    print("=" * 60)
    return None


def show_dataset_info():
    """æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯"""
    print("=" * 60)
    print("æ„å›¾æ ‡æ³¨æ•°æ®é›†ä¿¡æ¯")
    print("=" * 60)
    print("æ•°æ®é›†åç§°: äº¤äº’åœºæ™¯ä¸­çš„å¥å­æ„å›¾æ ‡æ³¨æ•°æ®")
    print("æ•°æ®é›† ID: DatatangBeijing/47811Sentence-IntentionAnnotationDataInInteractiveScenes")
    print("æ•°æ®é›†é¡µé¢: https://www.modelscope.cn/datasets/DatatangBeijing/47811Sentence-IntentionAnnotationDataInInteractiveScenes/quickstart")
    print("\næ•°æ®é›†ç‰¹ç‚¹:")
    print("- äº¤äº’åœºæ™¯ä¸­çš„å¥å­æ„å›¾æ ‡æ³¨")
    print("- é€‚åˆ Router æ¨¡å‹è®­ç»ƒï¼ˆæ„å›¾è¯†åˆ«/å·¥å…·è·¯ç”±ï¼‰")
    print("- åŒ…å«å¥å­å’Œå¯¹åº”çš„æ„å›¾æ ‡ç­¾")
    print("- ç¬¦åˆ 1.md ä¸­æè¿°çš„ Router ä»»åŠ¡éœ€æ±‚")
    print("\nä¸ Router ä»»åŠ¡çš„åŒ¹é…åº¦:")
    print("âœ“ æ„å›¾è¯†åˆ«ä»»åŠ¡")
    print("âœ“ äº¤äº’åœºæ™¯æ•°æ®")
    print("âœ“ å¯ç”¨äºè®­ç»ƒå·¥å…·è·¯ç”±æ¨¡å‹")
    print("\nè®¿é—®æ–¹å¼:")
    print("1. ç›´æ¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æ•°æ®é›†é¡µé¢æŸ¥çœ‹è¯¦æƒ…")
    print("2. ä½¿ç”¨ä»£ç åŠ è½½æ•°æ®é›†è¿›è¡Œè®­ç»ƒ")
    print("=" * 60)


def test_alternative_datasets():
    """æµ‹è¯•æ›¿ä»£æ•°æ®é›†æ˜¯å¦å¯ç”¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•æ›¿ä»£æ•°æ®é›†ï¼ˆé€‚åˆ Router è®­ç»ƒï¼‰")
    print("=" * 60)
    
    alternative_datasets = [
        ("snips_built_in_intents", "SNIPS æ„å›¾è¯†åˆ«æ•°æ®é›†"),
        ("atis", "ATIS èˆªç­ä¿¡æ¯æŸ¥è¯¢æ„å›¾"),
        ("multi_woz_v22", "MultiWOZ å¤šè½®å¯¹è¯æ•°æ®é›†"),
    ]
    
    from datasets import load_dataset
    
    for dataset_id, description in alternative_datasets:
        try:
            print(f"\næµ‹è¯•: {description} ({dataset_id})")
            dataset = load_dataset(dataset_id, cache_dir="./datasets")
            print(f"âœ… åŠ è½½æˆåŠŸ!")
            if hasattr(dataset, 'keys'):
                print(f"   å¯ç”¨åˆ†å‰²: {list(dataset.keys())}")
                for split in dataset.keys():
                    if len(dataset[split]) > 0:
                        print(f"   {split} å¤§å°: {len(dataset[split])}")
                        # æ˜¾ç¤ºä¸€ä¸ªç¤ºä¾‹
                        sample = dataset[split][0]
                        print(f"   ç¤ºä¾‹å­—æ®µ: {list(sample.keys())}")
                        break
            return dataset_id, dataset
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {str(e)[:100]}")
            continue
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ æ›¿ä»£æ•°æ®é›†å»ºè®®")
    print("=" * 60)
    print("\n1. è‹±æ–‡æ„å›¾è¯†åˆ«æ•°æ®é›†ï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰:")
    print("   - SNIPS: æ„å›¾è¯†åˆ«æ•°æ®é›†")
    print("   - ATIS: èˆªç­ä¿¡æ¯æŸ¥è¯¢æ„å›¾")
    print("   - MultiWOZ: å¤šè½®å¯¹è¯æ•°æ®é›†")
    print("\n2. ä¸­æ–‡æ•°æ®é›†ï¼ˆéœ€è¦æŸ¥æ‰¾ï¼‰:")
    print("   - CLUE å­ä»»åŠ¡ï¼ˆä¸­æ–‡è¯­è¨€ç†è§£è¯„ä¼°ï¼‰")
    print("   - ä¸­æ–‡å¯¹è¯æ•°æ®é›†")
    print("\n3. ä½¿ç”¨ç¤ºä¾‹:")
    print("   from datasets import load_dataset")
    print("   dataset = load_dataset('snips_built_in_intents')")
    print("=" * 60)
    return None, None


def suggest_alternative_datasets():
    """å»ºè®®æ›¿ä»£æ•°æ®é›†"""
    print("\n" + "=" * 60)
    print("ğŸ’¡ æ›¿ä»£æ•°æ®é›†å»ºè®®ï¼ˆé€‚åˆ Router è®­ç»ƒï¼‰")
    print("=" * 60)
    print("\n1. è‹±æ–‡æ„å›¾è¯†åˆ«æ•°æ®é›†ï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰:")
    print("   - SNIPS: æ„å›¾è¯†åˆ«æ•°æ®é›†")
    print("   - ATIS: èˆªç­ä¿¡æ¯æŸ¥è¯¢æ„å›¾")
    print("   - MultiWOZ: å¤šè½®å¯¹è¯æ•°æ®é›†")
    print("\n2. ä¸­æ–‡æ•°æ®é›†ï¼ˆéœ€è¦æŸ¥æ‰¾ï¼‰:")
    print("   - CLUE å­ä»»åŠ¡ï¼ˆä¸­æ–‡è¯­è¨€ç†è§£è¯„ä¼°ï¼‰")
    print("   - ä¸­æ–‡å¯¹è¯æ•°æ®é›†")
    print("\n3. ä½¿ç”¨ç¤ºä¾‹:")
    print("   from datasets import load_dataset")
    print("   dataset = load_dataset('snips_built_in_intents')")
    print("=" * 60)


if __name__ == "__main__":
    show_dataset_info()
    print("\n")
    dataset = load_intention_dataset()
    
    if dataset:
        print("\nâœ… æ•°æ®é›†åŠ è½½æˆåŠŸ!")
        if hasattr(dataset, 'keys'):
            print(f"å¯ç”¨æ•°æ®é›†åˆ†å‰²: {list(dataset.keys())}")
            # æ˜¾ç¤ºæ¯ä¸ªåˆ†å‰²çš„æ ·æœ¬
            for split in dataset.keys():
                if len(dataset[split]) > 0:
                    print(f"\n{split} æ•°æ®é›†ç¤ºä¾‹ (å‰3ä¸ªæ ·æœ¬):")
                    for i, sample in enumerate(dataset[split][:3]):
                        print(f"\næ ·æœ¬ {i+1}:")
                        print(sample)
                    print(f"\n{split} æ•°æ®é›†å¤§å°: {len(dataset[split])}")
                    break
        elif isinstance(dataset, dict):
            print(f"æ•°æ®é›†ç»“æ„: {list(dataset.keys())}")
            for key, value in dataset.items():
                if hasattr(value, '__len__'):
                    print(f"{key}: {len(value)} ä¸ªæ ·æœ¬")
                    if len(value) > 0:
                        print(f"ç¤ºä¾‹: {value[0]}")
                        break
        elif isinstance(dataset, (str, Path)):
            print(f"æ•°æ®é›†æ–‡ä»¶è·¯å¾„: {dataset}")
            print("è¯·æ‰‹åŠ¨è§£ææ•°æ®é›†æ–‡ä»¶")
    else:
        # æµ‹è¯•æ›¿ä»£æ•°æ®é›†
        alt_dataset_id, alt_dataset = test_alternative_datasets()
        if alt_dataset:
            print(f"\n" + "=" * 60)
            print(f"âœ… æ‰¾åˆ°å¯ç”¨çš„æ›¿ä»£æ•°æ®é›†: {alt_dataset_id}")
            print("=" * 60)
            print("å¯ä»¥ä½¿ç”¨æ­¤æ•°æ®é›†è¿›è¡Œ Router æ¨¡å‹è®­ç»ƒ")
            print("\næ•°æ®é›†ä¿¡æ¯:")
            if hasattr(alt_dataset, 'keys'):
                for split in alt_dataset.keys():
                    if len(alt_dataset[split]) > 0:
                        print(f"\n{split} æ•°æ®é›†:")
                        print(f"  å¤§å°: {len(alt_dataset[split])} ä¸ªæ ·æœ¬")
                        sample = alt_dataset[split][0]
                        print(f"  å­—æ®µ: {list(sample.keys())}")
                        print(f"  ç¤ºä¾‹:")
                        for key, value in sample.items():
                            print(f"    {key}: {value}")
                        break
            print("\nä½¿ç”¨æ–¹å¼:")
            print(f"  from datasets import load_dataset")
            print(f"  dataset = load_dataset('{alt_dataset_id}')")
            print("=" * 60)

